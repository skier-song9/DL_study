{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5b4dd5",
   "metadata": {},
   "source": [
    "### Fine-tuning Sentence Transformers Models\n",
    "> Reference : [Training&Fine-Tuning Sentence-Transformer Model](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/95_Training_Sentence_Transformers.ipynb) / [Sentence-Transformers Trainer, TrainingArguments Document](https://sbert.net/docs/package_reference/sentence_transformer/trainer.html#sentencetransformertrainer) <br>\n",
    "> Model : [google/embeddinggemma-300m](https://huggingface.co/google/embeddinggemma-300m)<br>\n",
    "> Dataset : [kakao KorSTS](https://github.com/kakaobrain/kor-nlu-datasets)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-transformers >= 4.56.0\n",
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "# !git clone https://github.com/kakaobrain/kor-nlu-datasets.git ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a649555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false' # set parallelism fasle in jupyter env or colab env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faafca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e09dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/song/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskiersong9\u001b[0m (\u001b[33mskiersong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)\n",
    "import wandb\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "533115a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,) (4, 768)\n",
      "tensor([[0.6963, 0.8456, 0.7681, 0.7857]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Download from the 🤗 Hub\n",
    "base_model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# Run inference with queries and documents\n",
    "query = \"Which planet is known as the Red Planet?\"\n",
    "documents = [\n",
    "    \"Venus is often called Earth's twin because of its similar size and proximity.\",\n",
    "    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n",
    "    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n",
    "    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n",
    "]\n",
    "query_embeddings = base_model.encode(query)\n",
    "document_embeddings = base_model.encode(documents)\n",
    "print(query_embeddings.shape, document_embeddings.shape)\n",
    "# (768,) (4, 768)\n",
    "\n",
    "# Compute similarities to determine a ranking\n",
    "similarities = base_model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarities)\n",
    "# tensor([[0.3011, 0.6359, 0.4930, 0.4889]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f37ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre        0\n",
      "filename     0\n",
      "year         0\n",
      "id           0\n",
      "score        0\n",
      "sentence1    0\n",
      "sentence2    5\n",
      "dtype: int64\n",
      "genre        0\n",
      "filename     0\n",
      "year         0\n",
      "id           0\n",
      "score        0\n",
      "sentence1    0\n",
      "sentence2    1\n",
      "dtype: int64\n",
      "genre        0\n",
      "filename     0\n",
      "year         0\n",
      "id           0\n",
      "score        0\n",
      "sentence1    0\n",
      "sentence2    3\n",
      "dtype: int64\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "data_dir = '../../data/kor-nlu-datasets/KorSTS/'\n",
    "train_df = pd.read_csv(os.path.join(data_dir,\"sts-train.tsv\"), sep='\\t',on_bad_lines=\"skip\")\n",
    "dev_df = pd.read_csv(os.path.join(data_dir,\"sts-dev.tsv\"), sep='\\t',on_bad_lines=\"skip\")\n",
    "test_df = pd.read_csv(os.path.join(data_dir,\"sts-test.tsv\"), sep='\\t',on_bad_lines=\"skip\")\n",
    "\n",
    "# check for NaN\n",
    "print(train_df.isnull().sum())\n",
    "print(dev_df.isnull().sum())\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# fillna with empty string\n",
    "train_df = train_df.fillna(\"\")\n",
    "print(train_df.isnull().sum().sum())\n",
    "dev_df = dev_df.fillna(\"\")\n",
    "print(dev_df.isnull().sum().sum())\n",
    "test_df = test_df.fillna(\"\")\n",
    "print(test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebed2a",
   "metadata": {},
   "source": [
    "### Check baseline Score for EmbeddingGemma-300M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9fa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_hf_dataset_from_df(df: pd.DataFrame) -> Dataset:\n",
    "    df['label'] = df['score'].apply(lambda x: x/5.0) # regularize score from [0,5] to [0,1]\n",
    "\n",
    "    # 1. Pandas DataFrame을 기본 Dataset으로 변환\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # 'sentence1'과 'sentence2' 컬럼 이름은 그대로 유지됩니다.\n",
    "    \n",
    "    # 필요한 최종 컬럼만 선택\n",
    "    # Trainer가 요구하는 기본 컬럼 구조: ['sentence1', 'sentence2', 'label']\n",
    "    hf_dataset = hf_dataset.select_columns(['sentence1', 'sentence2', 'label'])\n",
    "    \n",
    "    return hf_dataset\n",
    "train_hf_dataset = create_hf_dataset_from_df(train_df)\n",
    "dev_hf_dataset = create_hf_dataset_from_df(dev_df)\n",
    "test_hf_dataset = create_hf_dataset_from_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae9da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'korsts-dev_pearson_cosine': 0.35650364903426973,\n",
       " 'korsts-dev_spearman_cosine': 0.3698784068522297}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences1 = test_df['sentence1'].tolist()\n",
    "test_sentences2 = test_df['sentence2'].tolist()\n",
    "test_scores = test_df['score'].tolist()\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    test_sentences1, \n",
    "    test_sentences2, \n",
    "    test_scores,\n",
    "    main_similarity='cosine',\n",
    "    name='korsts-dev',\n",
    ")\n",
    "test_results = test_evaluator(base_model)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8533ac",
   "metadata": {},
   "source": [
    "### SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc375b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_epochs': 3,\n",
    "    'train_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'model_save_path': './outputs/embedding_gemma_300m_KorSTS',\n",
    "    'max_steps':800,\n",
    "}\n",
    "from argparse import Namespace\n",
    "config = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences1 = dev_df['sentence1'].tolist()\n",
    "dev_sentences2 = dev_df['sentence2'].tolist()\n",
    "dev_scores = dev_df['score'].tolist()\n",
    "evaluator = EmbeddingSimilarityEvaluator( # validation evaluator\n",
    "    dev_sentences1, \n",
    "    dev_sentences2, \n",
    "    dev_scores, \n",
    "    name='korsts-dev',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f114e62",
   "metadata": {},
   "source": [
    "#### Sentence-Transformers Losses\n",
    "- CosineSimilarityLoss\n",
    "    - 정의 : 회귀 기반 loss\n",
    "    - dataset composition : (sentence1, sentence2, similarity score)\n",
    "    - loss : $MSE(GT,\\text{sim}(\\mathbf{s1}, \\mathbf{s2}))$\n",
    "    - 특징 : similarity score 를 바탕으로 하는 regression loss.\n",
    "    - 단점 : s1-s2 문장 쌍만을 활용하기 때문에 배치 내의 다른 문장들과의 negatives를 활용하지 못한다.\n",
    "- TripletLoss\n",
    "    - 정의 : 거리 기반 loss\n",
    "    - dataset composition : (query_sentence, positive_sentence, negative_sentence)\n",
    "    - loss : $\\max(0, ||\\mathbf{q} - \\mathbf{p}||^2 - ||\\mathbf{q}-\\mathbf{n}||^2+\\alpha) $ >> query-positive 임베딩 벡터 간 거리와 query-negative 임베딩 벡터 간 거리의 차이가 alpha(=margin)보다 클 때, 즉 query가 negative와 더 가까울 때 loss가 발생하여 positive 쪽으로 임베딩하도록 유도한다.\n",
    "    - 특징 : 두 문장의 상대적인 관게(Q가 N보다 P에 가까워야 함)를 명확히 학습할 수 있다.\n",
    "    - 단점 : 데이터 구축이 어렵다. 마찬가지로 배치 내의 다른 negative 문장을 활용하지 않으므로 효율이 떨어진다.\n",
    "    - 발전된 loss : MultipleNegativeRankingLoss\n",
    "- MultipleNegativesRankingLoss (=InfoNCE)\n",
    "    - 정의 : batch 기반 cross-entropy loss\n",
    "    - dataset composition : (sentence1, sentence2) . 두 문장은 항상 의미적으로 '유사'한 문장이다.\n",
    "    - loss : $-\\log\\frac{\\exp(s(q,d^+)/\\tau)}{\\sum_{i=1}^N \\exp(s(q,d^i)/\\tau)}$\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = losses.CosineSimilarityLoss(model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=config.model_save_path,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    max_steps=config.max_steps,\n",
    "    per_device_train_batch_size=config.train_batch_size,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=2e-6,\n",
    "    lr_scheduler_type='cosine',\n",
    "    # lr_scheduler_kwargs=None,\n",
    "    warmup_steps=0,\n",
    "    #Logging & Saving\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    # OOM\n",
    "    auto_find_batch_size=True,\n",
    "    torch_empty_cache_steps=20,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    # report\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"steps{config.max_steps}_B{config.train_batch_size}_GA{config.gradient_accumulation_steps}\",    \n",
    "    # HF Hub \n",
    "    push_to_hub=True,\n",
    "    hub_model_id='song9/embeddinggemma-300m-KorSTS',\n",
    "    hub_strategy='end',\n",
    "    hub_token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf_dataset,     # Dataset \n",
    "    eval_dataset=dev_hf_dataset,       # Dataset \n",
    "    loss=train_loss,            # 손실 함수 (CosineSimilarityLoss)\n",
    "    evaluator=evaluator               # 평가자 (EmbeddingSimilarityEvaluator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2feb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences1 = test_df['sentence1'].tolist()\n",
    "test_sentences2 = test_df['sentence2'].tolist()\n",
    "test_scores = test_df['score'].tolist()\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    test_sentences1, \n",
    "    test_sentences2, \n",
    "    test_scores,\n",
    "    main_similarity='cosine',\n",
    "    name='korsts-test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_evaluator(trainer.model)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7034d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "repo_id = \"song9/embeddinggemma-300m-KorSTS\"\n",
    "huggingface_hub.create_repo(\n",
    "    repo_id, \n",
    "    exist_ok=True,\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\n",
    "    commit_message=\"Initial Commit\",\n",
    "    token=HF_TOKEN, # hf token\n",
    "    repo_id=repo_id, # set HF repo id : {username}/{repo_name}\n",
    "    language=[\"multilingual\",\"ko\"], # must be lowercase and ISO 639-1 format\n",
    "    license=\"cc-by-sa-4.0\", # must be lowercase\n",
    "    tags=[\"STS\"],\n",
    "    model_name=repo_id, # {hf_username}/{repo_name} > used for sample code\n",
    "    finetuned_from=\"google/embeddinggemma-300m\",\n",
    "    tasks=\"Sentence Similarity\",\n",
    "    dataset=\"kakao/KorSTS\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
